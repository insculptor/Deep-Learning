{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><b>Gesture Recognition</b></center></h1>\n",
    "\n",
    "<div style=\"text-align: right\"> Submitted By:\n",
    "<br>Deepika Hariharan \\ Ravi Dhir</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* [1. Introduction](#1)\n",
    " * [1.1 Import the Libraries](#1.1)\n",
    "* [2. Generator](#2)\n",
    " * [2.1 Defining CSV File Parameters](#2.1)\n",
    " * [2.2 Defining Generator](#2.2)\n",
    " * [2.3 Define a function to train and evaluate the model](#2.3)\n",
    " * [2.4 Defining Parameters](#2.4)\n",
    "* [3.  Architecture 1 - 3D Conv](#3)\n",
    " * [3.1 Model 1 : CNN + GRU-RNN](#3.1)\n",
    " * [3.2 Model 2 : CNN + GRU-RNN with Normalization technique : percentile](#3.2) \n",
    " * [3.3 Model 3 : CNN + GRU-RNN with Optimiser : Adadelta](#3.3) \n",
    " * [3.3 Model 4 : CNN + GRU-RNN - Image size changes](#3.4) \n",
    "* [4. Architecture 2 - Transfer Learning + GRU(RNN)](#4)\n",
    " * [4.1 Model 5 : VGG16 Transfer Learning + GRU-RNN](#4.1)\n",
    " * [4.2 Model 6 : RESNET50 Transfer Learning + GRU-RNN](#4.2) \n",
    "* [5. Architecture 3 - Conv3D](#5)\n",
    " * [4.1 Model 7 : Conv3D](#5.1)\n",
    " * [4.2 Model 8 : 3D CNN - Kernel, dropouts and Image size change](#5.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Introduction\n",
    "\n",
    "We are working as a data scientist at a *home electronics* company which manufactures state of the art smart televisions. We want to develop a cool feature in the smart-TV that can **recognise five different gestures performed by the user** which will help users control the TV without using a remote.\n",
    "\n",
    "The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "\n",
    "- **Thumbs up:**  Increase the volume\n",
    "- **Thumbs down:** Decrease the volume\n",
    "- **Left swipe:** 'Jump' backwards 10 seconds\n",
    "- **Right swipe:** 'Jump' forward 10 seconds  \n",
    "- **Stop:** Pause the movie\n",
    "\n",
    "In this group project, we are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. \n",
    "Let's import the following libraries to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "### 1.1 Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imresize\n",
    "from  imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import skimage\n",
    "\n",
    "from scipy.misc.pilutil import imread\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, GRU\n",
    "from keras.layers.convolutional import Conv3D,Conv2D, MaxPooling3D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)\n",
    "#tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_doc = np.random.permutation(open('/notebooks/storage/Final_data/Collated_training/train.csv').readlines())\n",
    "#val_doc = np.random.permutation(open('/notebooks/storage/Final_data/Collated_training/val.csv').readlines())\n",
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Examine an image\n",
    "img_path = 'Project_data/train/'+ train_doc[5].split(';')[0]\n",
    "img = imread(img_path + '/'+ os.listdir(img_path)[0])\n",
    "plt.imshow(img)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, frames_per_video, frames_red, img_x, img_y, norm_technique='minmax'):\n",
    "    \n",
    "    # General Generator function that can be used for other textual data also\n",
    "    # Source_path is root data directory\n",
    "    # Folder_list - .csv file that we pass (traninng\\validation)\n",
    "    # Batch_size -  size of the batch\n",
    "    # frames_per_video - Number of image indexes that will be used\n",
    "    # frames_red - Frame reduction factor , 1 : keeps all frames, 2 : keeps alternate images\n",
    "    # img_x ,img_y - Resolution of final image\n",
    "    # norm_technique - Image Normalisation Technique\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    #create a list of image numbers you want to use for a particular video\n",
    "    img_idx = np.arange(0,frames_per_video, frames_red) \n",
    "    num_imgs = len(img_idx)\n",
    "\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        \n",
    "        num_full_batches = int(len(folder_list)/batch_size)\n",
    "        num_batches = num_full_batches if (len(folder_list)%batch_size == 0) else (num_full_batches+1)\n",
    "        \n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            \n",
    "            # Update batch_size to handle remaining datapoints which are left after full batches\n",
    "            if (num_batches == (batch + 1) and (len(folder_list) > num_full_batches * batch_size)):\n",
    "                data_points = len(folder_list) - (num_full_batches * batch_size)\n",
    "            else:\n",
    "                data_points = batch_size\n",
    "            \n",
    "            batch_data = np.zeros((data_points, num_imgs, img_x, img_y, 3)) #3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((data_points,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            for folder in range(data_points): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "            \n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    height, width , channel = image.shape\n",
    "\n",
    "                    # CROP : Resulting in low validation accuracy\n",
    "                    # if size of image is 160x120, we will need to crop it make is AxA image\n",
    "                    #if height != width:\n",
    "                    #    image = image[:120, 20:140,:]\n",
    "\n",
    "                    # RESIZE\n",
    "                    # We can now resize all images to have same dimension \n",
    "                    #image = cv2.resize(image, (img_x, img_y))\n",
    "                    image = skimage.transform.resize(image,(img_x,img_y))\n",
    "\n",
    "                    # NORMALIZATION\n",
    "                    #Create Batch Data Channelwise (RGB):\n",
    "                    if norm_technique == 'minmax':\n",
    "                        image = (image - np.min(image))/(np.max(image)- np.min(image))\n",
    "                    else :\n",
    "                        image = (image - np.percentile(image,5))/(np.percentile(image,95) - np.percentile(image,5))\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]#normalise and feed in the image\n",
    "     \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "##  3. Architecture 1 - CNN + GRU-RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "### 3.1 Model 1: CNN + GRU-RNN \n",
    "Batch size: 30 Epochs: 25 <br> \n",
    "Image size: 100 * 100, Frames per video : 15, Optimiser : Adam, Normalization : minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_10 (TimeDis (None, 15, 50, 50, 8)     224       \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 15, 50, 50, 16)    1168      \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 15, 25, 25, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 15, 25, 25, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 15, 12, 12, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 15, 12, 12, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 15, 6, 6, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 15, 6, 6, 64)      256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15, 6, 6, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 15, 2304)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 15, 128)           295040    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               74112     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 402,837\n",
      "Trainable params: 402,709\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  Project_data/val ; batch size = 30\n",
      "Source path =  Project_data/train ; batch size = 30\n",
      "Epoch 1/25\n",
      "23/23 [==============================] - 67s 3s/step - loss: 1.3798 - categorical_accuracy: 0.3760 - val_loss: 1.3659 - val_categorical_accuracy: 0.4300\n",
      "Epoch 2/25\n",
      "23/23 [==============================] - 58s 3s/step - loss: 1.0581 - categorical_accuracy: 0.5819 - val_loss: 1.1743 - val_categorical_accuracy: 0.5200\n",
      "Epoch 3/25\n",
      "23/23 [==============================] - 54s 2s/step - loss: 0.8360 - categorical_accuracy: 0.6609 - val_loss: 1.6633 - val_categorical_accuracy: 0.4400\n",
      "Epoch 4/25\n",
      "23/23 [==============================] - 56s 2s/step - loss: 0.7040 - categorical_accuracy: 0.7320 - val_loss: 2.1932 - val_categorical_accuracy: 0.3800\n",
      "Epoch 5/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.5799 - categorical_accuracy: 0.8002 - val_loss: 1.4225 - val_categorical_accuracy: 0.5700\n",
      "Epoch 6/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.3941 - categorical_accuracy: 0.8467 - val_loss: 1.6292 - val_categorical_accuracy: 0.5500\n",
      "Epoch 7/25\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.3174 - categorical_accuracy: 0.8897 - val_loss: 2.1131 - val_categorical_accuracy: 0.5100\n",
      "Epoch 8/25\n",
      "23/23 [==============================] - 55s 2s/step - loss: 0.2759 - categorical_accuracy: 0.9091 - val_loss: 2.6972 - val_categorical_accuracy: 0.4800\n",
      "Epoch 9/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.2141 - categorical_accuracy: 0.9274 - val_loss: 1.7199 - val_categorical_accuracy: 0.5500\n",
      "Epoch 10/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.1252 - categorical_accuracy: 0.9489 - val_loss: 2.5858 - val_categorical_accuracy: 0.5100\n",
      "Epoch 11/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.2605 - categorical_accuracy: 0.9173 - val_loss: 2.5121 - val_categorical_accuracy: 0.5700\n",
      "Epoch 12/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.1078 - categorical_accuracy: 0.9652 - val_loss: 1.3045 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.0410 - categorical_accuracy: 0.9898 - val_loss: 1.2344 - val_categorical_accuracy: 0.7100\n",
      "Epoch 14/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.0472 - categorical_accuracy: 0.9884 - val_loss: 1.1814 - val_categorical_accuracy: 0.7100\n",
      "Epoch 15/25\n",
      "23/23 [==============================] - 57s 2s/step - loss: 0.0505 - categorical_accuracy: 0.9802 - val_loss: 1.1605 - val_categorical_accuracy: 0.7200\n",
      "Epoch 16/25\n",
      "23/23 [==============================] - 57s 2s/step - loss: 0.1040 - categorical_accuracy: 0.9634 - val_loss: 1.1561 - val_categorical_accuracy: 0.7100\n",
      "Epoch 17/25\n",
      "23/23 [==============================] - 54s 2s/step - loss: 0.0302 - categorical_accuracy: 0.9956 - val_loss: 1.1720 - val_categorical_accuracy: 0.7100\n",
      "Epoch 18/25\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.0318 - categorical_accuracy: 0.9942 - val_loss: 1.1779 - val_categorical_accuracy: 0.7200\n",
      "Epoch 19/25\n",
      "23/23 [==============================] - 51s 2s/step - loss: 0.0303 - categorical_accuracy: 0.9942 - val_loss: 1.1846 - val_categorical_accuracy: 0.7100\n",
      "Epoch 20/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.0461 - categorical_accuracy: 0.9788 - val_loss: 1.1877 - val_categorical_accuracy: 0.7100\n",
      "Epoch 21/25\n",
      "23/23 [==============================] - 55s 2s/step - loss: 0.0269 - categorical_accuracy: 0.9956 - val_loss: 1.1820 - val_categorical_accuracy: 0.7200\n",
      "Epoch 22/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.0267 - categorical_accuracy: 0.9956 - val_loss: 1.1878 - val_categorical_accuracy: 0.7200\n",
      "Epoch 23/25\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.0217 - categorical_accuracy: 0.9971 - val_loss: 1.1942 - val_categorical_accuracy: 0.7100\n",
      "Epoch 24/25\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.0244 - categorical_accuracy: 0.9971 - val_loss: 1.2030 - val_categorical_accuracy: 0.7000\n",
      "Epoch 25/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.0462 - categorical_accuracy: 0.9831 - val_loss: 1.2078 - val_categorical_accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5a58135f60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator parameters\n",
    "num_epochs = 25\n",
    "batch_size = 30\n",
    "frames_per_video = 30\n",
    "frame_red = 2\n",
    "img_x = 100\n",
    "img_y = 100\n",
    "norm = 'minmax'\n",
    "\n",
    "#Model Architecture\n",
    "num_classes = 5\n",
    "\n",
    "#optimizer\n",
    "opt = Adam(0.001)\n",
    "\n",
    "# Work on smaller dataset for initial experiments by randomly pick a subset of t for train v for validation\n",
    "# num_epochs = 20\n",
    "# t,v = 30,20\n",
    "# train_doc = train_doc[ np.random.choice(train_doc.shape[0], t, replace=False)]\n",
    "# val_doc = val_doc[ np.random.choice(val_doc.shape[0], v, replace=False)]\n",
    "# num_train_sequences = len(train_doc)\n",
    "# num_val_sequences = len(val_doc)\n",
    "\n",
    "# Input shape\n",
    "input_shape=(frames_per_video//frame_red, img_x, img_y, 3)\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(8, (3,3), strides=(2, 2),activation='relu', padding='same'), input_shape=input_shape))\n",
    "model.add(TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "## GRU based RNN with softmax\n",
    "model.add(GRU(128, return_sequences=False))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile and print the model summary\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n",
    "\n",
    "#checkpoints and callbacks\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=10, cooldown=5, verbose=1, mode='auto',min_delta=0.0001)\n",
    "#For initial experiments not checkpointing\n",
    "#callbacks_list = [checkpoint, LR]\n",
    "callbacks_list = [LR]\n",
    "\n",
    "#create the generator for fit_generator\n",
    "train_generator = generator(train_path, train_doc, batch_size, frames_per_video, frame_red, img_x, img_y, norm)\n",
    "val_generator = generator(val_path, val_doc, batch_size, frames_per_video, frame_red, img_x, img_y, norm)\n",
    "\n",
    "# The steps_per_epoch and validation_steps are used by fit_generator to decide the number of next() calls it need to make.\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "        \n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "### 3.2 Model 2: CNN + GRU-RNN with Normalization technique : percentile\n",
    "Batch size: 30 Epochs: 25 <br> \n",
    "Image size: 100 * 100, Frames per video : 15, Optimiser : Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 15, 50, 50, 8)     224       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 15, 50, 50, 16)    1168      \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 15, 25, 25, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 15, 25, 25, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 15, 12, 12, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 15, 12, 12, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 15, 6, 6, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 15, 6, 6, 64)      256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 6, 6, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 15, 2304)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15, 128)           295040    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               74112     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 402,837\n",
      "Trainable params: 402,709\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  Project_data/val ; batch size = 30\n",
      "Source path =  Project_data/train ; batch size = 30\n",
      "Epoch 1/25\n",
      "23/23 [==============================] - 180s 8s/step - loss: 1.5545 - categorical_accuracy: 0.2773 - val_loss: 1.3619 - val_categorical_accuracy: 0.5400\n",
      "Epoch 2/25\n",
      "23/23 [==============================] - 77s 3s/step - loss: 1.1760 - categorical_accuracy: 0.5267 - val_loss: 1.5599 - val_categorical_accuracy: 0.3700\n",
      "Epoch 3/25\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.9132 - categorical_accuracy: 0.6382 - val_loss: 1.7327 - val_categorical_accuracy: 0.4400\n",
      "Epoch 4/25\n",
      "23/23 [==============================] - 61s 3s/step - loss: 0.8604 - categorical_accuracy: 0.6774 - val_loss: 1.4682 - val_categorical_accuracy: 0.5000\n",
      "Epoch 5/25\n",
      "23/23 [==============================] - 61s 3s/step - loss: 0.5456 - categorical_accuracy: 0.7924 - val_loss: 1.3522 - val_categorical_accuracy: 0.5700\n",
      "Epoch 6/25\n",
      "23/23 [==============================] - 61s 3s/step - loss: 0.2965 - categorical_accuracy: 0.8969 - val_loss: 1.4875 - val_categorical_accuracy: 0.6300\n",
      "Epoch 7/25\n",
      "23/23 [==============================] - 66s 3s/step - loss: 0.1663 - categorical_accuracy: 0.9425 - val_loss: 1.6302 - val_categorical_accuracy: 0.6300\n",
      "Epoch 8/25\n",
      "23/23 [==============================] - 67s 3s/step - loss: 0.3098 - categorical_accuracy: 0.9106 - val_loss: 1.9233 - val_categorical_accuracy: 0.5200\n",
      "Epoch 9/25\n",
      "23/23 [==============================] - 61s 3s/step - loss: 0.2927 - categorical_accuracy: 0.8998 - val_loss: 2.3522 - val_categorical_accuracy: 0.5400\n",
      "Epoch 10/25\n",
      "23/23 [==============================] - 64s 3s/step - loss: 0.1264 - categorical_accuracy: 0.9695 - val_loss: 1.8673 - val_categorical_accuracy: 0.5600\n",
      "Epoch 11/25\n",
      "23/23 [==============================] - 69s 3s/step - loss: 0.0566 - categorical_accuracy: 0.9811 - val_loss: 1.7322 - val_categorical_accuracy: 0.5800\n",
      "Epoch 12/25\n",
      "23/23 [==============================] - 62s 3s/step - loss: 0.0237 - categorical_accuracy: 0.9942 - val_loss: 1.7995 - val_categorical_accuracy: 0.6400\n",
      "Epoch 13/25\n",
      "23/23 [==============================] - 71s 3s/step - loss: 0.0140 - categorical_accuracy: 0.9985 - val_loss: 1.7488 - val_categorical_accuracy: 0.6200\n",
      "Epoch 14/25\n",
      "23/23 [==============================] - 71s 3s/step - loss: 0.0120 - categorical_accuracy: 0.9956 - val_loss: 2.1222 - val_categorical_accuracy: 0.6200\n",
      "Epoch 15/25\n",
      "23/23 [==============================] - 71s 3s/step - loss: 0.0118 - categorical_accuracy: 0.9985 - val_loss: 2.1495 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 16/25\n",
      "23/23 [==============================] - 68s 3s/step - loss: 0.0061 - categorical_accuracy: 1.0000 - val_loss: 2.1303 - val_categorical_accuracy: 0.6000\n",
      "Epoch 17/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.0027 - categorical_accuracy: 1.0000 - val_loss: 2.1069 - val_categorical_accuracy: 0.6100\n",
      "Epoch 18/25\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.0035 - categorical_accuracy: 1.0000 - val_loss: 2.0867 - val_categorical_accuracy: 0.6100\n",
      "Epoch 19/25\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.0050 - categorical_accuracy: 0.9985 - val_loss: 2.0642 - val_categorical_accuracy: 0.6100\n",
      "Epoch 20/25\n",
      "23/23 [==============================] - 61s 3s/step - loss: 0.0039 - categorical_accuracy: 1.0000 - val_loss: 2.0460 - val_categorical_accuracy: 0.6100\n",
      "Epoch 21/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.0033 - categorical_accuracy: 1.0000 - val_loss: 2.0326 - val_categorical_accuracy: 0.6200\n",
      "Epoch 22/25\n",
      "23/23 [==============================] - 69s 3s/step - loss: 0.0036 - categorical_accuracy: 1.0000 - val_loss: 2.0246 - val_categorical_accuracy: 0.6200\n",
      "Epoch 23/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.0032 - categorical_accuracy: 1.0000 - val_loss: 2.0178 - val_categorical_accuracy: 0.6100\n",
      "Epoch 24/25\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.0032 - categorical_accuracy: 1.0000 - val_loss: 2.0114 - val_categorical_accuracy: 0.6100\n",
      "Epoch 25/25\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.0039 - categorical_accuracy: 1.0000 - val_loss: 2.0057 - val_categorical_accuracy: 0.6100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc78c23c9e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator parameters\n",
    "num_epochs = 25\n",
    "batch_size = 30\n",
    "frames_per_video = 30\n",
    "frame_red = 2\n",
    "img_x = 100\n",
    "img_y = 100\n",
    "norm = 'percentile'\n",
    "\n",
    "#Model Architecture\n",
    "num_classes = 5\n",
    "\n",
    "#optimizer\n",
    "opt = Adam(0.001)\n",
    "\n",
    "# Work on smaller dataset for initial experiments by randomly pick a subset of t for train v for validation\n",
    "# num_epochs = 20\n",
    "# t,v = 30,20\n",
    "# train_doc = train_doc[ np.random.choice(train_doc.shape[0], t, replace=False)]\n",
    "# val_doc = val_doc[ np.random.choice(val_doc.shape[0], v, replace=False)]\n",
    "# num_train_sequences = len(train_doc)\n",
    "# num_val_sequences = len(val_doc)\n",
    "\n",
    "\n",
    "# Input shape\n",
    "input_shape=(frames_per_video//frame_red, img_x, img_y, 3)\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(8, (3,3), strides=(2, 2),activation='relu', padding='same'), input_shape=input_shape))\n",
    "model.add(TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "## GRU based RNN with softmax\n",
    "model.add(GRU(128, return_sequences=False))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile and print the model summary\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n",
    "\n",
    "#checkpoints and callbacks\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=10, cooldown=5, verbose=1, mode='auto',min_delta=0.0001)\n",
    "#For initial experiments not checkpointing\n",
    "#callbacks_list = [checkpoint, LR]\n",
    "callbacks_list = [LR]\n",
    "\n",
    "#create the generator for fit_generator\n",
    "train_generator = generator(train_path, train_doc, batch_size, frames_per_video, frame_red, img_x, img_y, norm)\n",
    "val_generator = generator(val_path, val_doc, batch_size, frames_per_video, frame_red, img_x, img_y, norm)\n",
    "\n",
    "# The steps_per_epoch and validation_steps are used by fit_generator to decide the number of next() calls it need to make.\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "        \n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "### Model 3: CNN + GRU-RNN with Optimiser : Adadelta\n",
    "Batch size: 30 Epochs: 35 <br> \n",
    "Image size: 100 * 100, Frames per video : 15, Normalization : minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_19 (TimeDis (None, 15, 50, 50, 8)     224       \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 15, 50, 50, 16)    1168      \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 15, 25, 25, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 15, 25, 25, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 15, 12, 12, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 15, 12, 12, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 15, 6, 6, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 15, 6, 6, 64)      256       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 15, 6, 6, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 15, 2304)          0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 15, 128)           295040    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 15, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 128)               74112     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 402,837\n",
      "Trainable params: 402,709\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  Project_data/val Source path =  Project_data/train ; batch size = 30\n",
      "; batch size = 30\n",
      "Epoch 1/35\n",
      "23/23 [==============================] - 64s 3s/step - loss: 1.4263 - categorical_accuracy: 0.3667 - val_loss: 1.9788 - val_categorical_accuracy: 0.2900\n",
      "Epoch 2/35\n",
      "23/23 [==============================] - 53s 2s/step - loss: 1.1567 - categorical_accuracy: 0.5244 - val_loss: 1.7620 - val_categorical_accuracy: 0.4500\n",
      "Epoch 3/35\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.9993 - categorical_accuracy: 0.5955 - val_loss: 3.2282 - val_categorical_accuracy: 0.2300\n",
      "Epoch 4/35\n",
      "23/23 [==============================] - 56s 2s/step - loss: 0.8353 - categorical_accuracy: 0.6710 - val_loss: 2.4083 - val_categorical_accuracy: 0.4100\n",
      "Epoch 5/35\n",
      "23/23 [==============================] - 50s 2s/step - loss: 0.7488 - categorical_accuracy: 0.7015 - val_loss: 2.3392 - val_categorical_accuracy: 0.3300\n",
      "Epoch 6/35\n",
      "23/23 [==============================] - 53s 2s/step - loss: 0.5313 - categorical_accuracy: 0.7895 - val_loss: 1.9568 - val_categorical_accuracy: 0.5000\n",
      "Epoch 7/35\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.3576 - categorical_accuracy: 0.8641 - val_loss: 3.3148 - val_categorical_accuracy: 0.3400\n",
      "Epoch 8/35\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.3326 - categorical_accuracy: 0.8766 - val_loss: 1.5942 - val_categorical_accuracy: 0.6000\n",
      "Epoch 9/35\n",
      "23/23 [==============================] - 56s 2s/step - loss: 0.1699 - categorical_accuracy: 0.9492 - val_loss: 1.4468 - val_categorical_accuracy: 0.6300\n",
      "Epoch 10/35\n",
      "23/23 [==============================] - 50s 2s/step - loss: 0.0918 - categorical_accuracy: 0.9782 - val_loss: 3.2934 - val_categorical_accuracy: 0.4700\n",
      "Epoch 11/35\n",
      "23/23 [==============================] - 54s 2s/step - loss: 0.0771 - categorical_accuracy: 0.9782 - val_loss: 2.2203 - val_categorical_accuracy: 0.5500\n",
      "Epoch 12/35\n",
      "23/23 [==============================] - 56s 2s/step - loss: 0.0306 - categorical_accuracy: 0.9956 - val_loss: 1.8722 - val_categorical_accuracy: 0.5900\n",
      "Epoch 13/35\n",
      "23/23 [==============================] - 51s 2s/step - loss: 0.0490 - categorical_accuracy: 0.9817 - val_loss: 2.0939 - val_categorical_accuracy: 0.6100\n",
      "Epoch 14/35\n",
      "23/23 [==============================] - 50s 2s/step - loss: 0.0319 - categorical_accuracy: 0.9898 - val_loss: 2.3335 - val_categorical_accuracy: 0.5600\n",
      "Epoch 15/35\n",
      "23/23 [==============================] - 51s 2s/step - loss: 0.0062 - categorical_accuracy: 1.0000 - val_loss: 1.7553 - val_categorical_accuracy: 0.6500\n",
      "Epoch 16/35\n",
      "23/23 [==============================] - 50s 2s/step - loss: 0.0040 - categorical_accuracy: 1.0000 - val_loss: 1.8062 - val_categorical_accuracy: 0.6600\n",
      "Epoch 17/35\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.0105 - categorical_accuracy: 0.9971 - val_loss: 2.0798 - val_categorical_accuracy: 0.6200\n",
      "Epoch 18/35\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.0208 - categorical_accuracy: 0.9956 - val_loss: 2.1630 - val_categorical_accuracy: 0.6700\n",
      "Epoch 19/35\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.0196 - categorical_accuracy: 0.9927 - val_loss: 2.6187 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.01.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc73f98e9e8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator parameters\n",
    "num_epochs = 35\n",
    "batch_size = 30\n",
    "frames_per_video = 30\n",
    "frame_red = 2\n",
    "img_x = 100\n",
    "img_y = 100\n",
    "norm = 'minmax'\n",
    "\n",
    "#Model Architecture\n",
    "num_classes = 5\n",
    "\n",
    "#optimizer\n",
    "opt = optimizers.Adadelta()\n",
    "\n",
    "# Work on smaller dataset for initial experiments by randomly pick a subset of t for train v for validation\n",
    "# num_epochs = 20\n",
    "# t,v = 30,20\n",
    "# train_doc = train_doc[ np.random.choice(train_doc.shape[0], t, replace=False)]\n",
    "# val_doc = val_doc[ np.random.choice(val_doc.shape[0], v, replace=False)]\n",
    "# num_train_sequences = len(train_doc)\n",
    "# num_val_sequences = len(val_doc)\n",
    "\n",
    "# Input shape\n",
    "input_shape=(frames_per_video//frame_red, img_x, img_y, 3)\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(8, (3,3), strides=(2, 2),activation='relu', padding='same'), input_shape=input_shape))\n",
    "model.add(TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "## GRU based RNN with softmax\n",
    "model.add(GRU(128, return_sequences=False))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile and print the model summary\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n",
    "\n",
    "#checkpoints and callbacks\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=10, cooldown=5, verbose=1, mode='auto',min_delta=0.0001)\n",
    "\n",
    "ES = EarlyStopping(monitor='val_loss', patience=10, verbose=0)\n",
    "#For initial experiments not checkpointing\n",
    "#callbacks_list = [checkpoint, LR, ES]\n",
    "callbacks_list = [LR, ES]\n",
    "\n",
    "#create the generator for fit_generator\n",
    "train_generator = generator(train_path, train_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "val_generator = generator(val_path, val_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "\n",
    "# The steps_per_epoch and validation_steps are used by fit_generator to decide the number of next() calls it need to make.\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a>\n",
    "### 3.4 Model 4: CNN + GRU-RNN - Image size changes\n",
    "Batch size: 30 Epochs: 35 <br> \n",
    "Image size: 120 * 120, Frames per video : 30, normalization : minmax, Optimizer : Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_28 (TimeDis (None, 30, 60, 60, 8)     224       \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 30, 60, 60, 16)    1168      \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 30, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 30, 30, 30, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 30, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, 30, 15, 15, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, 30, 7, 7, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_35 (TimeDis (None, 30, 7, 7, 64)      256       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 30, 7, 7, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_36 (TimeDis (None, 30, 3136)          0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 30, 128)           401536    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 30, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 128)               74112     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 509,333\n",
      "Trainable params: 509,205\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path = Source path =  Project_data/train ; batch size = 30\n",
      " Project_data/val ; batch size = 30\n",
      "Epoch 1/35\n",
      "23/23 [==============================] - 233s 10s/step - loss: 1.3879 - categorical_accuracy: 0.3885 - val_loss: 1.6198 - val_categorical_accuracy: 0.3800\n",
      "Epoch 2/35\n",
      "23/23 [==============================] - 125s 5s/step - loss: 1.1572 - categorical_accuracy: 0.5221 - val_loss: 2.8067 - val_categorical_accuracy: 0.3600\n",
      "Epoch 3/35\n",
      "23/23 [==============================] - 132s 6s/step - loss: 0.9336 - categorical_accuracy: 0.6179 - val_loss: 1.4593 - val_categorical_accuracy: 0.3900\n",
      "Epoch 4/35\n",
      "23/23 [==============================] - 121s 5s/step - loss: 0.7442 - categorical_accuracy: 0.7117 - val_loss: 2.5673 - val_categorical_accuracy: 0.3600\n",
      "Epoch 5/35\n",
      "23/23 [==============================] - 127s 6s/step - loss: 0.6593 - categorical_accuracy: 0.7607 - val_loss: 1.9269 - val_categorical_accuracy: 0.4700\n",
      "Epoch 6/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.5046 - categorical_accuracy: 0.8206 - val_loss: 2.9193 - val_categorical_accuracy: 0.3400\n",
      "Epoch 7/35\n",
      "23/23 [==============================] - 119s 5s/step - loss: 0.3563 - categorical_accuracy: 0.8752 - val_loss: 0.9068 - val_categorical_accuracy: 0.7300\n",
      "Epoch 8/35\n",
      "23/23 [==============================] - 122s 5s/step - loss: 0.2043 - categorical_accuracy: 0.9361 - val_loss: 2.0572 - val_categorical_accuracy: 0.5200\n",
      "Epoch 9/35\n",
      "23/23 [==============================] - 129s 6s/step - loss: 0.1319 - categorical_accuracy: 0.9521 - val_loss: 1.9302 - val_categorical_accuracy: 0.5300\n",
      "Epoch 10/35\n",
      "23/23 [==============================] - 120s 5s/step - loss: 0.0762 - categorical_accuracy: 0.9840 - val_loss: 1.0678 - val_categorical_accuracy: 0.7200\n",
      "Epoch 11/35\n",
      "23/23 [==============================] - 118s 5s/step - loss: 0.0413 - categorical_accuracy: 0.9956 - val_loss: 2.8330 - val_categorical_accuracy: 0.5000\n",
      "Epoch 12/35\n",
      "23/23 [==============================] - 129s 6s/step - loss: 0.0662 - categorical_accuracy: 0.9797 - val_loss: 1.2669 - val_categorical_accuracy: 0.7200\n",
      "Epoch 13/35\n",
      "23/23 [==============================] - 125s 5s/step - loss: 0.0151 - categorical_accuracy: 0.9985 - val_loss: 1.2887 - val_categorical_accuracy: 0.7500\n",
      "Epoch 14/35\n",
      "23/23 [==============================] - 109s 5s/step - loss: 0.0081 - categorical_accuracy: 1.0000 - val_loss: 1.8925 - val_categorical_accuracy: 0.6600\n",
      "Epoch 15/35\n",
      "23/23 [==============================] - 123s 5s/step - loss: 0.0130 - categorical_accuracy: 0.9971 - val_loss: 1.6754 - val_categorical_accuracy: 0.7000\n",
      "Epoch 16/35\n",
      "23/23 [==============================] - 118s 5s/step - loss: 0.0034 - categorical_accuracy: 1.0000 - val_loss: 1.8063 - val_categorical_accuracy: 0.6200\n",
      "Epoch 17/35\n",
      "23/23 [==============================] - 110s 5s/step - loss: 0.0035 - categorical_accuracy: 1.0000 - val_loss: 1.5601 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.01.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc72b2d1198>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator parameters\n",
    "num_epochs = 35\n",
    "batch_size = 30\n",
    "frames_per_video = 30\n",
    "frame_red = 1\n",
    "img_x = 120\n",
    "img_y = 120\n",
    "norm = 'minmax'\n",
    "\n",
    "#Model Architecture\n",
    "num_classes = 5\n",
    "\n",
    "#optimizer\n",
    "opt = optimizers.Adadelta()\n",
    "\n",
    "# Work on smaller dataset for initial experiments by randomly pick a subset of t for train v for validation\n",
    "# num_epochs = 20\n",
    "# t,v = 30,20\n",
    "# train_doc = train_doc[ np.random.choice(train_doc.shape[0], t, replace=False)]\n",
    "# val_doc = val_doc[ np.random.choice(val_doc.shape[0], v, replace=False)]\n",
    "# num_train_sequences = len(train_doc)\n",
    "# num_val_sequences = len(val_doc)\n",
    "\n",
    "# Input shape\n",
    "input_shape=(frames_per_video//frame_red, img_x, img_y, 3)\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(8, (3,3), strides=(2, 2),activation='relu', padding='same'), input_shape=input_shape))\n",
    "model.add(TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "## GRU based RNN with softmax\n",
    "model.add(GRU(128, return_sequences=False))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile and print the model summary\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n",
    "\n",
    "#checkpoints and callbacks\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=10, cooldown=5, verbose=1, mode='auto',min_delta=0.0001)\n",
    "\n",
    "ES = EarlyStopping(monitor='val_loss', patience=10, verbose=0)\n",
    "#For initial experiments not checkpointing\n",
    "#callbacks_list = [checkpoint, LR, ES]\n",
    "callbacks_list = [LR, ES]\n",
    "\n",
    "#create the generator for fit_generator\n",
    "train_generator = generator(train_path, train_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "val_generator = generator(val_path, val_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "\n",
    "# The steps_per_epoch and validation_steps are used by fit_generator to decide the number of next() calls it need to make.\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "##  4. Architecture 2 - Transfer Learning + GRU-RNN Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "### 4.1 Model 5: VGG16 Transfer Learning + GRU-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_3 (TimeDist (None, 30, 3, 3, 512)     14714688  \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 4608)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30, 128)           589952    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 15,411,909\n",
      "Trainable params: 697,221\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  Project_data/val ; batch size = 30\n",
      "Source path =  Project_data/train Epoch 1/35\n",
      "; batch size = 30\n",
      "23/23 [==============================] - 163s 7s/step - loss: 1.6848 - categorical_accuracy: 0.2120 - val_loss: 1.6039 - val_categorical_accuracy: 0.2300\n",
      "Epoch 2/35\n",
      "23/23 [==============================] - 123s 5s/step - loss: 1.5584 - categorical_accuracy: 0.3223 - val_loss: 1.6050 - val_categorical_accuracy: 0.2100\n",
      "Epoch 3/35\n",
      "23/23 [==============================] - 125s 5s/step - loss: 1.3427 - categorical_accuracy: 0.4483 - val_loss: 1.4274 - val_categorical_accuracy: 0.4500\n",
      "Epoch 4/35\n",
      "23/23 [==============================] - 123s 5s/step - loss: 1.2345 - categorical_accuracy: 0.4765 - val_loss: 2.5057 - val_categorical_accuracy: 0.1800\n",
      "Epoch 5/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 1.1780 - categorical_accuracy: 0.5003 - val_loss: 1.3530 - val_categorical_accuracy: 0.4900\n",
      "Epoch 6/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 1.0227 - categorical_accuracy: 0.5883 - val_loss: 1.5360 - val_categorical_accuracy: 0.4200\n",
      "Epoch 7/35\n",
      "23/23 [==============================] - 124s 5s/step - loss: 0.9176 - categorical_accuracy: 0.6542 - val_loss: 1.7297 - val_categorical_accuracy: 0.4600\n",
      "Epoch 8/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.7848 - categorical_accuracy: 0.6855 - val_loss: 1.6026 - val_categorical_accuracy: 0.4700\n",
      "Epoch 9/35\n",
      "23/23 [==============================] - 124s 5s/step - loss: 0.7192 - categorical_accuracy: 0.7465 - val_loss: 1.2006 - val_categorical_accuracy: 0.4600\n",
      "Epoch 10/35\n",
      "23/23 [==============================] - 124s 5s/step - loss: 0.6605 - categorical_accuracy: 0.7561 - val_loss: 1.1924 - val_categorical_accuracy: 0.6000\n",
      "Epoch 11/35\n",
      "23/23 [==============================] - 129s 6s/step - loss: 0.5595 - categorical_accuracy: 0.7988 - val_loss: 2.2031 - val_categorical_accuracy: 0.4300\n",
      "Epoch 12/35\n",
      "23/23 [==============================] - 128s 6s/step - loss: 0.4789 - categorical_accuracy: 0.8360 - val_loss: 1.4141 - val_categorical_accuracy: 0.5800\n",
      "Epoch 13/35\n",
      "23/23 [==============================] - 127s 6s/step - loss: 0.3693 - categorical_accuracy: 0.8781 - val_loss: 1.1045 - val_categorical_accuracy: 0.6100\n",
      "Epoch 14/35\n",
      "23/23 [==============================] - 124s 5s/step - loss: 0.2981 - categorical_accuracy: 0.8911 - val_loss: 0.8083 - val_categorical_accuracy: 0.7700\n",
      "Epoch 15/35\n",
      "23/23 [==============================] - 127s 6s/step - loss: 0.3832 - categorical_accuracy: 0.8786 - val_loss: 2.6986 - val_categorical_accuracy: 0.4500\n",
      "Epoch 16/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.2775 - categorical_accuracy: 0.9071 - val_loss: 0.7627 - val_categorical_accuracy: 0.7700\n",
      "Epoch 17/35\n",
      "23/23 [==============================] - 129s 6s/step - loss: 0.1448 - categorical_accuracy: 0.9535 - val_loss: 0.7430 - val_categorical_accuracy: 0.7900\n",
      "Epoch 18/35\n",
      "23/23 [==============================] - 127s 6s/step - loss: 0.1205 - categorical_accuracy: 0.9652 - val_loss: 1.6444 - val_categorical_accuracy: 0.6400\n",
      "Epoch 19/35\n",
      "23/23 [==============================] - 124s 5s/step - loss: 0.1497 - categorical_accuracy: 0.9521 - val_loss: 1.2438 - val_categorical_accuracy: 0.7000\n",
      "Epoch 20/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.0804 - categorical_accuracy: 0.9768 - val_loss: 1.7010 - val_categorical_accuracy: 0.6800\n",
      "Epoch 21/35\n",
      "23/23 [==============================] - 122s 5s/step - loss: 0.2051 - categorical_accuracy: 0.9347 - val_loss: 1.1961 - val_categorical_accuracy: 0.7800\n",
      "Epoch 22/35\n",
      "23/23 [==============================] - 125s 5s/step - loss: 0.0858 - categorical_accuracy: 0.9739 - val_loss: 1.1029 - val_categorical_accuracy: 0.7400\n",
      "Epoch 23/35\n",
      "23/23 [==============================] - 121s 5s/step - loss: 0.0417 - categorical_accuracy: 0.9913 - val_loss: 0.9898 - val_categorical_accuracy: 0.7900\n",
      "Epoch 24/35\n",
      "23/23 [==============================] - 122s 5s/step - loss: 0.0202 - categorical_accuracy: 0.9971 - val_loss: 1.1693 - val_categorical_accuracy: 0.7700\n",
      "Epoch 25/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.0449 - categorical_accuracy: 0.9884 - val_loss: 0.9922 - val_categorical_accuracy: 0.7900\n",
      "Epoch 26/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.0342 - categorical_accuracy: 0.9927 - val_loss: 2.5173 - val_categorical_accuracy: 0.6700\n",
      "Epoch 27/35\n",
      "23/23 [==============================] - 125s 5s/step - loss: 0.2003 - categorical_accuracy: 0.9550 - val_loss: 1.7271 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.01.\n",
      "Epoch 28/35\n",
      "23/23 [==============================] - 129s 6s/step - loss: 0.0254 - categorical_accuracy: 0.9942 - val_loss: 1.6918 - val_categorical_accuracy: 0.7300\n",
      "Epoch 29/35\n",
      "23/23 [==============================] - 121s 5s/step - loss: 0.0234 - categorical_accuracy: 0.9956 - val_loss: 1.6631 - val_categorical_accuracy: 0.7300\n",
      "Epoch 30/35\n",
      "23/23 [==============================] - 125s 5s/step - loss: 0.0228 - categorical_accuracy: 0.9956 - val_loss: 1.6394 - val_categorical_accuracy: 0.7300\n",
      "Epoch 31/35\n",
      "23/23 [==============================] - 124s 5s/step - loss: 0.0177 - categorical_accuracy: 0.9985 - val_loss: 1.6201 - val_categorical_accuracy: 0.7300\n",
      "Epoch 32/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.0212 - categorical_accuracy: 0.9985 - val_loss: 1.5988 - val_categorical_accuracy: 0.7300\n",
      "Epoch 33/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.0183 - categorical_accuracy: 0.9956 - val_loss: 1.5839 - val_categorical_accuracy: 0.7400\n",
      "Epoch 34/35\n",
      "23/23 [==============================] - 129s 6s/step - loss: 0.0195 - categorical_accuracy: 0.9956 - val_loss: 1.5698 - val_categorical_accuracy: 0.7400\n",
      "Epoch 35/35\n",
      "23/23 [==============================] - 128s 6s/step - loss: 0.0115 - categorical_accuracy: 0.9985 - val_loss: 1.5460 - val_categorical_accuracy: 0.7400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0e9865a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator parameters\n",
    "num_epochs = 35\n",
    "batch_size = 30\n",
    "frames_per_video = 30\n",
    "frame_red = 1\n",
    "img_x = 120\n",
    "img_y = 120\n",
    "norm = 'minmax'\n",
    "\n",
    "#Model Architecture\n",
    "num_classes = 5\n",
    "\n",
    "#optimizer\n",
    "opt = optimizers.Adadelta()\n",
    "\n",
    "# Work on smaller dataset for initial experiments by randomly pick a subset of t for train v for validation\n",
    "# num_epochs = 20\n",
    "# t,v = 30,20\n",
    "# train_doc = train_doc[ np.random.choice(train_doc.shape[0], t, replace=False)]\n",
    "# val_doc = val_doc[ np.random.choice(val_doc.shape[0], v, replace=False)]\n",
    "# num_train_sequences = len(train_doc)\n",
    "# num_val_sequences = len(val_doc)\n",
    "\n",
    "# Input shape\n",
    "input_shape=(frames_per_video//frame_red, img_x, img_y, 3)\n",
    "\n",
    "vgg = VGG16(include_top=False, weights='imagenet', input_shape=(img_x, img_y, 3))\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(vgg, input_shape= input_shape ))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GRU(128, return_sequences=False))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "#compile and print the model summary\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n",
    "\n",
    "#checkpoints and callbacks\n",
    "model_name = 'model_transfer_rnn' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=10, cooldown=5, verbose=1, mode='auto',min_delta=0.0001)\n",
    "#For initial experiments on lower data size no need to checkpoint\n",
    "#callbacks_list = [checkpoint, LR]\n",
    "callbacks_list = [LR]\n",
    "\n",
    "#create the generator for fit_generator\n",
    "train_generator = generator(train_path, train_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "val_generator = generator(val_path, val_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "\n",
    "# The steps_per_epoch and validation_steps are used by fit_generator to decide the number of next() calls it need to make.\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "### 4.2 Model 6: RestNet50 Transfer Learning + GRU-RNN\n",
    "Batch size: 30 Epochs: 35 <br> \n",
    "Image size: 200 * 200 (ResNet restriction for image size greater than 197x197) <br>\n",
    "Frames per video : 30, Normalization : minmax, Optimizer:Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_9 (TimeDist (None, 30, 7, 7, 2048)    23587712  \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 30, 100352)        0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 30, 64)            6422592   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 64)                24768     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 30,039,557\n",
      "Trainable params: 6,451,845\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  Project_data/val ; batch size = 30\n",
      "Source path =  Epoch 1/20\n",
      "Project_data/train ; batch size = 30\n",
      "1/1 [==============================] - 35s 35s/step - loss: 1.6901 - categorical_accuracy: 0.1667 - val_loss: 2.0966 - val_categorical_accuracy: 0.1500\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 11s 11s/step - loss: 1.9970 - categorical_accuracy: 0.4000 - val_loss: 1.6470 - val_categorical_accuracy: 0.2000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 11s 11s/step - loss: 1.5077 - categorical_accuracy: 0.3000 - val_loss: 1.7079 - val_categorical_accuracy: 0.1500\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 11s 11s/step - loss: 1.7397 - categorical_accuracy: 0.2667 - val_loss: 1.8337 - val_categorical_accuracy: 0.2000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 11s 11s/step - loss: 1.5737 - categorical_accuracy: 0.4000 - val_loss: 1.8512 - val_categorical_accuracy: 0.2500\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.7598 - categorical_accuracy: 0.3333 - val_loss: 1.8037 - val_categorical_accuracy: 0.2000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.6548 - categorical_accuracy: 0.4000 - val_loss: 1.7340 - val_categorical_accuracy: 0.2000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.5067 - categorical_accuracy: 0.3667 - val_loss: 1.8772 - val_categorical_accuracy: 0.2000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.3939 - categorical_accuracy: 0.3667 - val_loss: 1.8834 - val_categorical_accuracy: 0.2000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.1681 - categorical_accuracy: 0.4333 - val_loss: 1.8182 - val_categorical_accuracy: 0.2000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.2568 - categorical_accuracy: 0.5333 - val_loss: 1.7621 - val_categorical_accuracy: 0.1500\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.5728 - categorical_accuracy: 0.3667 - val_loss: 1.8665 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.01.\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.4208 - categorical_accuracy: 0.5333 - val_loss: 1.8654 - val_categorical_accuracy: 0.2000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.3229 - categorical_accuracy: 0.4667 - val_loss: 1.8658 - val_categorical_accuracy: 0.2000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.5837 - categorical_accuracy: 0.3333 - val_loss: 1.8674 - val_categorical_accuracy: 0.2000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.3813 - categorical_accuracy: 0.4333 - val_loss: 1.8689 - val_categorical_accuracy: 0.2000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.5806 - categorical_accuracy: 0.3667 - val_loss: 1.8653 - val_categorical_accuracy: 0.1500\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.3725 - categorical_accuracy: 0.4667 - val_loss: 1.8672 - val_categorical_accuracy: 0.1500\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.2848 - categorical_accuracy: 0.5333 - val_loss: 1.8650 - val_categorical_accuracy: 0.1500\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.4489 - categorical_accuracy: 0.5000 - val_loss: 1.8643 - val_categorical_accuracy: 0.1500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe078263400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator parameters\n",
    "num_epochs = 35\n",
    "batch_size = 30\n",
    "frames_per_video = 30\n",
    "frame_red = 1\n",
    "img_x = 200\n",
    "img_y = 200\n",
    "norm = 'minmax'\n",
    "\n",
    "#Model Architecture\n",
    "num_classes = 5\n",
    "\n",
    "#optimizer\n",
    "opt = optimizers.Adadelta()\n",
    "\n",
    "# Work on smaller dataset for initial experiments by randomly pick a subset of t for train v for validation\n",
    "num_epochs = 20\n",
    "t,v = 30,20\n",
    "train_doc = train_doc[ np.random.choice(train_doc.shape[0], t, replace=False)]\n",
    "val_doc = val_doc[ np.random.choice(val_doc.shape[0], v, replace=False)]\n",
    "num_train_sequences = len(train_doc)\n",
    "num_val_sequences = len(val_doc)\n",
    "\n",
    "# Input shape\n",
    "input_shape=(frames_per_video//frame_red, img_x, img_y, 3)\n",
    "\n",
    "# Import a RESNET model, consider input image with shape (img_x, img_y, 3)\n",
    "base = ResNet50(include_top=False, weights='imagenet', input_shape=(img_x,img_y,3))\n",
    "\n",
    "# Mark only four last layers as trainable\n",
    "for layer in base.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(base, input_shape= input_shape ))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GRU(64, return_sequences=False))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "#compile and print the model summary\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n",
    "\n",
    "#checkpoints and callbacks\n",
    "model_name = 'model_transfer_rnn' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=10, cooldown=5, verbose=1, mode='auto',min_delta=0.0001)\n",
    "#For initial experiments on lower data size no need to checkpoint\n",
    "#callbacks_list = [checkpoint, LR]\n",
    "callbacks_list = [LR]\n",
    "\n",
    "#create the generator for fit_generator\n",
    "train_generator = generator(train_path, train_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "val_generator = generator(val_path, val_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "\n",
    "# The steps_per_epoch and validation_steps are used by fit_generator to decide the number of next() calls it need to make.\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Architecture 3 - 3D CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1\"></a>\n",
    "### 5.1 Model 7: 3D CNN \n",
    "Batch size: 30 Epochs: 25 <br> \n",
    "Image size: 100 * 100, frames per video : 15, normalization : minmax, optimizer : Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 15, 100, 100, 8)   3008      \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 15, 100, 100, 16)  16016     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 7, 50, 50, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 50, 50, 32)     64032     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 3, 25, 25, 64)     256064    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,527,733\n",
      "Trainable params: 1,527,605\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  Project_data/val ; batch size = 30\n",
      "Source path =  Project_data/train ; batch size = 30\n",
      "Epoch 1/25\n",
      "23/23 [==============================] - 188s 8s/step - loss: 2.1009 - categorical_accuracy: 0.3275 - val_loss: 7.1675 - val_categorical_accuracy: 0.2100\n",
      "Epoch 2/25\n",
      "23/23 [==============================] - 55s 2s/step - loss: 1.3657 - categorical_accuracy: 0.4262 - val_loss: 1.5064 - val_categorical_accuracy: 0.4500\n",
      "Epoch 3/25\n",
      "23/23 [==============================] - 56s 2s/step - loss: 1.2616 - categorical_accuracy: 0.4867 - val_loss: 1.4667 - val_categorical_accuracy: 0.4800\n",
      "Epoch 4/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 1.2639 - categorical_accuracy: 0.4872 - val_loss: 8.6608 - val_categorical_accuracy: 0.2100\n",
      "Epoch 5/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 1.2799 - categorical_accuracy: 0.4852 - val_loss: 1.5991 - val_categorical_accuracy: 0.4200\n",
      "Epoch 6/25\n",
      "23/23 [==============================] - 57s 2s/step - loss: 1.2673 - categorical_accuracy: 0.5041 - val_loss: 2.3686 - val_categorical_accuracy: 0.4300\n",
      "Epoch 7/25\n",
      "23/23 [==============================] - 58s 3s/step - loss: 1.1970 - categorical_accuracy: 0.5476 - val_loss: 10.8427 - val_categorical_accuracy: 0.2300\n",
      "Epoch 8/25\n",
      "23/23 [==============================] - 57s 2s/step - loss: 1.2427 - categorical_accuracy: 0.5491 - val_loss: 4.5359 - val_categorical_accuracy: 0.3700\n",
      "Epoch 9/25\n",
      "23/23 [==============================] - 56s 2s/step - loss: 0.9422 - categorical_accuracy: 0.6396 - val_loss: 6.8759 - val_categorical_accuracy: 0.3200\n",
      "Epoch 10/25\n",
      "23/23 [==============================] - 57s 2s/step - loss: 1.0627 - categorical_accuracy: 0.6112 - val_loss: 2.2526 - val_categorical_accuracy: 0.2800\n",
      "Epoch 11/25\n",
      "23/23 [==============================] - 57s 2s/step - loss: 0.9207 - categorical_accuracy: 0.6376 - val_loss: 1.5836 - val_categorical_accuracy: 0.4700\n",
      "Epoch 12/25\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.8463 - categorical_accuracy: 0.6696 - val_loss: 2.1117 - val_categorical_accuracy: 0.4900\n",
      "Epoch 13/25\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.9223 - categorical_accuracy: 0.6745 - val_loss: 4.8251 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.01.\n",
      "Epoch 14/25\n",
      "23/23 [==============================] - 58s 3s/step - loss: 1.2031 - categorical_accuracy: 0.5917 - val_loss: 2.1732 - val_categorical_accuracy: 0.4100\n",
      "Epoch 15/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 1.0187 - categorical_accuracy: 0.6298 - val_loss: 1.4935 - val_categorical_accuracy: 0.6100\n",
      "Epoch 16/25\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.9378 - categorical_accuracy: 0.6342 - val_loss: 1.2027 - val_categorical_accuracy: 0.6400\n",
      "Epoch 17/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.9635 - categorical_accuracy: 0.6391 - val_loss: 1.0550 - val_categorical_accuracy: 0.6700\n",
      "Epoch 18/25\n",
      "23/23 [==============================] - 57s 2s/step - loss: 0.8805 - categorical_accuracy: 0.6396 - val_loss: 1.0351 - val_categorical_accuracy: 0.6500\n",
      "Epoch 19/25\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.8446 - categorical_accuracy: 0.6739 - val_loss: 0.9738 - val_categorical_accuracy: 0.6700\n",
      "Epoch 20/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.8615 - categorical_accuracy: 0.6527 - val_loss: 0.9392 - val_categorical_accuracy: 0.6500\n",
      "Epoch 21/25\n",
      "23/23 [==============================] - 57s 2s/step - loss: 0.7696 - categorical_accuracy: 0.6899 - val_loss: 0.9258 - val_categorical_accuracy: 0.6500\n",
      "Epoch 22/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.7381 - categorical_accuracy: 0.7082 - val_loss: 0.9187 - val_categorical_accuracy: 0.6400\n",
      "Epoch 23/25\n",
      "23/23 [==============================] - 57s 2s/step - loss: 0.7457 - categorical_accuracy: 0.7097 - val_loss: 0.9075 - val_categorical_accuracy: 0.6400\n",
      "Epoch 24/25\n",
      "23/23 [==============================] - 57s 2s/step - loss: 0.7588 - categorical_accuracy: 0.7088 - val_loss: 0.8945 - val_categorical_accuracy: 0.6400\n",
      "Epoch 25/25\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.7050 - categorical_accuracy: 0.7155 - val_loss: 0.8947 - val_categorical_accuracy: 0.6400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4fe1afe390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator parameters\n",
    "num_epochs = 25\n",
    "batch_size = 30\n",
    "frames_per_video = 30\n",
    "frame_red = 2\n",
    "img_x = 100\n",
    "img_y = 100\n",
    "norm = 'minmax'\n",
    "\n",
    "#Model Architecture\n",
    "num_classes = 5\n",
    "\n",
    "#optimizer\n",
    "opt = optimizers.Adadelta()\n",
    "\n",
    "# Work on smaller dataset for initial experiments by randomly pick a subset of t for train v for validation\n",
    "# num_epochs = 20\n",
    "# t,v = 30,20\n",
    "# train_doc = train_doc[ np.random.choice(train_doc.shape[0], t, replace=False)]\n",
    "# val_doc = val_doc[ np.random.choice(val_doc.shape[0], v, replace=False)]\n",
    "# num_train_sequences = len(train_doc)\n",
    "# num_val_sequences = len(val_doc)\n",
    "\n",
    "# Input shape\n",
    "input_shape=(frames_per_video//frame_red, img_x, img_y, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(8, (5,5,5), activation='relu', padding = 'same', input_shape=input_shape))\n",
    "model.add(Conv3D(16, (5,5,5), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D((2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(32, (5,5,5), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D((2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, (5,5,5), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile and print the model summary\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n",
    "\n",
    "#checkpoints and callbacks\n",
    "model_name = 'model_conv3d' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=10, cooldown=5, verbose=1, mode='auto',min_delta=0.0001)\n",
    "\n",
    "#ES = EarlyStopping(monitor='val_loss', patience=10, verbose=0)\n",
    "#For initial experiments not checkpointing\n",
    "#callbacks_list = [checkpoint, LR, ES]\n",
    "callbacks_list = [LR]\n",
    "\n",
    "#create the generator for fit_generator\n",
    "train_generator = generator(train_path, train_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "val_generator = generator(val_path, val_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "\n",
    "# The steps_per_epoch and validation_steps are used by fit_generator to decide the number of next() calls it need to make.\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "###  5.2 Model 8: 3D CNN  - Kernel, dropouts and Image size change\n",
    "Batch size: 30 Epochs: 35 <br> \n",
    "Image size: 120 * 120, frames per video : 30, normalization : minmax, optimizer : Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_29 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_23 (MaxPooling (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_30 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_31 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_25 (MaxPooling (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_32 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_26 (MaxPooling (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 864,101\n",
      "Trainable params: 863,989\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  Source path =  Project_data/train Project_data/val ; batch size = 30\n",
      "; batch size = 30\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 14s 14s/step - loss: 5.2986 - categorical_accuracy: 0.1333 - val_loss: 5.1990 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00001: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00001-5.29860-0.13333-5.19902-0.35000.h5\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 2s 2s/step - loss: 4.5189 - categorical_accuracy: 0.4000 - val_loss: 10.0897 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00002: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00002-4.51890-0.40000-10.08965-0.20000.h5\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 8.6977 - categorical_accuracy: 0.3000 - val_loss: 3.3684 - val_categorical_accuracy: 0.0500\n",
      "\n",
      "Epoch 00003: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00003-8.69767-0.30000-3.36843-0.05000.h5\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.5297 - categorical_accuracy: 0.2667 - val_loss: 6.1124 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00004: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00004-3.52972-0.26667-6.11239-0.20000.h5\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 4.3753 - categorical_accuracy: 0.2000 - val_loss: 2.8727 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00005: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00005-4.37533-0.20000-2.87270-0.35000.h5\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.5093 - categorical_accuracy: 0.4000 - val_loss: 3.3191 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00006: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00006-2.50927-0.40000-3.31913-0.20000.h5\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.9413 - categorical_accuracy: 0.4333 - val_loss: 1.6896 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00007: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00007-2.94134-0.43333-1.68957-0.20000.h5\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.4392 - categorical_accuracy: 0.5000 - val_loss: 1.5516 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00008: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00008-1.43924-0.50000-1.55157-0.40000.h5\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.9148 - categorical_accuracy: 0.5667 - val_loss: 1.7506 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00009: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00009-0.91482-0.56667-1.75065-0.20000.h5\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.6352 - categorical_accuracy: 0.7667 - val_loss: 1.5872 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00010: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00010-0.63518-0.76667-1.58723-0.40000.h5\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.7732 - categorical_accuracy: 0.7000 - val_loss: 1.7422 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00011: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00011-0.77318-0.70000-1.74215-0.25000.h5\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.8040 - categorical_accuracy: 0.7000 - val_loss: 1.6192 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00012: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00012-0.80400-0.70000-1.61924-0.50000.h5\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.5654 - categorical_accuracy: 0.7667 - val_loss: 1.4542 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00013: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00013-0.56541-0.76667-1.45423-0.25000.h5\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.7788 - categorical_accuracy: 0.6667 - val_loss: 1.4920 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00014: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00014-0.77883-0.66667-1.49200-0.35000.h5\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.4392 - categorical_accuracy: 0.8667 - val_loss: 1.6025 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00015: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00015-0.43919-0.86667-1.60247-0.35000.h5\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.4823 - categorical_accuracy: 0.8333 - val_loss: 1.5298 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00016: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00016-0.48235-0.83333-1.52985-0.35000.h5\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.2328 - categorical_accuracy: 0.9667 - val_loss: 1.6214 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00017: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00017-0.23277-0.96667-1.62137-0.40000.h5\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.2221 - categorical_accuracy: 0.9667 - val_loss: 1.6485 - val_categorical_accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00018-0.22212-0.96667-1.64847-0.35000.h5\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.2798 - categorical_accuracy: 0.9667 - val_loss: 1.5257 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00019: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00019-0.27983-0.96667-1.52571-0.30000.h5\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.2074 - categorical_accuracy: 0.9333 - val_loss: 1.5269 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00020: saving model to model_conv3d_final_2020-03-3012_03_20.885493/model-00020-0.20736-0.93333-1.52686-0.35000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8860324240>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator parameters\n",
    "num_epochs = 25\n",
    "batch_size = 30\n",
    "frames_per_video = 30\n",
    "frame_red = 1\n",
    "img_x = 120\n",
    "img_y = 120\n",
    "norm = 'minmax'\n",
    "\n",
    "#Model Architecture\n",
    "num_classes = 5\n",
    "\n",
    "#optimizer\n",
    "#opt = Adam(0.001)\n",
    "opt = optimizers.Adadelta()\n",
    "\n",
    "# Work on smaller dataset for initial experiments by randomly pick a subset of t for train v for validation\n",
    "num_epochs = 20\n",
    "t,v = 30,20\n",
    "train_doc = train_doc[ np.random.choice(train_doc.shape[0], t, replace=False)]\n",
    "val_doc = val_doc[ np.random.choice(val_doc.shape[0], v, replace=False)]\n",
    "num_train_sequences = len(train_doc)\n",
    "num_val_sequences = len(val_doc)\n",
    "\n",
    "# Input shape\n",
    "input_shape=(frames_per_video//frame_red,img_x,img_y,3)\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, (3,3,3), input_shape=input_shape, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2))) \n",
    "\n",
    "model.add(Conv3D(32, (1,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, (1,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "            \n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "            \n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile and print the model summary\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n",
    "\n",
    "#checkpoints and callbacks\n",
    "model_name = 'model_conv3d_final' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=10, cooldown=5, verbose=1, mode='auto',min_delta=0.0001)\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "#create the generator for fit_generator\n",
    "train_generator = generator(train_path, train_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "val_generator = generator(val_path, val_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "\n",
    "#The steps_per_epoch and validation_steps are used by fit_generator to decide the number of next() calls it need to make.\n",
    "steps_per_epoch = math.ceil(num_train_sequences/batch_size)\n",
    "validation_steps = math.ceil(num_val_sequences/batch_size)\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D CNN  Final Model\n",
    "Batch size: 30 Epochs: 35 <br> \n",
    "Image size: 120 * 120, 30 frames per video, minmax normalization, Adadelta optimizer<br>\n",
    "Refer to other architecture parameters at the top of the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 864,101\n",
      "Trainable params: 863,989\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  Project_data/val ; batch size = 30\n",
      "Source path =  Project_data/train ; batch size = 30\n",
      "Epoch 1/35\n",
      "23/23 [==============================] - 245s 11s/step - loss: 2.3853 - categorical_accuracy: 0.2903 - val_loss: 1.3654 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00001: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00001-2.37722-0.30166-1.36538-0.33000.h5\n",
      "Epoch 2/35\n",
      "23/23 [==============================] - 114s 5s/step - loss: 1.3133 - categorical_accuracy: 0.4271 - val_loss: 1.1794 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00002: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00002-1.31463-0.41780-1.17940-0.51000.h5\n",
      "Epoch 3/35\n",
      "23/23 [==============================] - 125s 5s/step - loss: 1.2449 - categorical_accuracy: 0.4567 - val_loss: 1.1204 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00003: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00003-1.21333-0.46154-1.12044-0.58000.h5\n",
      "Epoch 4/35\n",
      "23/23 [==============================] - 124s 5s/step - loss: 1.0316 - categorical_accuracy: 0.5665 - val_loss: 0.9258 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00004: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00004-1.04653-0.56259-0.92583-0.60000.h5\n",
      "Epoch 5/35\n",
      "23/23 [==============================] - 116s 5s/step - loss: 0.9499 - categorical_accuracy: 0.6173 - val_loss: 1.2584 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00005: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00005-0.96763-0.61538-1.25841-0.56000.h5\n",
      "Epoch 6/35\n",
      "23/23 [==============================] - 121s 5s/step - loss: 0.9299 - categorical_accuracy: 0.6603 - val_loss: 0.8925 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00006: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00006-0.94530-0.64706-0.89250-0.63000.h5\n",
      "Epoch 7/35\n",
      "23/23 [==============================] - 124s 5s/step - loss: 0.6612 - categorical_accuracy: 0.7422 - val_loss: 1.6670 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00007: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00007-0.65712-0.74510-1.66698-0.32000.h5\n",
      "Epoch 8/35\n",
      "23/23 [==============================] - 116s 5s/step - loss: 0.5843 - categorical_accuracy: 0.7692 - val_loss: 1.0312 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00008: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00008-0.59780-0.76018-1.03115-0.59000.h5\n",
      "Epoch 9/35\n",
      "23/23 [==============================] - 130s 6s/step - loss: 0.5404 - categorical_accuracy: 0.8031 - val_loss: 2.1111 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00009: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00009-0.51444-0.80845-2.11106-0.41000.h5\n",
      "Epoch 10/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.3995 - categorical_accuracy: 0.8432 - val_loss: 0.7483 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00010: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00010-0.41256-0.83710-0.74830-0.69000.h5\n",
      "Epoch 11/35\n",
      "23/23 [==============================] - 124s 5s/step - loss: 0.3903 - categorical_accuracy: 0.8568 - val_loss: 1.9807 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00011: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00011-0.38167-0.86425-1.98075-0.49000.h5\n",
      "Epoch 12/35\n",
      "23/23 [==============================] - 113s 5s/step - loss: 0.3304 - categorical_accuracy: 0.8868 - val_loss: 0.7058 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00012: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00012-0.34266-0.88235-0.70578-0.74000.h5\n",
      "Epoch 13/35\n",
      "23/23 [==============================] - 125s 5s/step - loss: 0.2359 - categorical_accuracy: 0.9071 - val_loss: 1.2554 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00013: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00013-0.23754-0.90347-1.25537-0.55000.h5\n",
      "Epoch 14/35\n",
      "23/23 [==============================] - 131s 6s/step - loss: 0.1578 - categorical_accuracy: 0.9463 - val_loss: 0.9334 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00014: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00014-0.16371-0.94419-0.93340-0.71000.h5\n",
      "Epoch 15/35\n",
      "23/23 [==============================] - 130s 6s/step - loss: 0.1504 - categorical_accuracy: 0.9410 - val_loss: 5.3769 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00015: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00015-0.13891-0.95173-5.37693-0.36000.h5\n",
      "Epoch 16/35\n",
      "23/23 [==============================] - 128s 6s/step - loss: 0.2736 - categorical_accuracy: 0.9027 - val_loss: 0.9262 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00016: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00016-0.27503-0.89894-0.92620-0.69000.h5\n",
      "Epoch 17/35\n",
      "23/23 [==============================] - 121s 5s/step - loss: 0.2035 - categorical_accuracy: 0.9314 - val_loss: 2.5102 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00017: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00017-0.12213-0.95475-2.51023-0.40000.h5\n",
      "Epoch 18/35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 126s 5s/step - loss: 0.1688 - categorical_accuracy: 0.9594 - val_loss: 0.8843 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00018: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00018-0.17173-0.95777-0.88432-0.76000.h5\n",
      "Epoch 19/35\n",
      "23/23 [==============================] - 122s 5s/step - loss: 0.0834 - categorical_accuracy: 0.9527 - val_loss: 1.3221 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00019: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00019-0.07428-0.96380-1.32206-0.64000.h5\n",
      "Epoch 20/35\n",
      "23/23 [==============================] - 122s 5s/step - loss: 0.0690 - categorical_accuracy: 0.9768 - val_loss: 1.0223 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00020: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00020-0.07137-0.97587-1.02226-0.72000.h5\n",
      "Epoch 21/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.0342 - categorical_accuracy: 0.9913 - val_loss: 1.0655 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00021: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00021-0.03443-0.99095-1.06553-0.67000.h5\n",
      "Epoch 22/35\n",
      "23/23 [==============================] - 124s 5s/step - loss: 0.0465 - categorical_accuracy: 0.9855 - val_loss: 0.8291 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00022: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00022-0.04834-0.98492-0.82905-0.75000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.01.\n",
      "Epoch 23/35\n",
      "23/23 [==============================] - 113s 5s/step - loss: 0.0245 - categorical_accuracy: 0.9956 - val_loss: 0.8196 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00023: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00023-0.02324-0.99548-0.81956-0.75000.h5\n",
      "Epoch 24/35\n",
      "23/23 [==============================] - 126s 5s/step - loss: 0.0261 - categorical_accuracy: 0.9927 - val_loss: 0.8118 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00024: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00024-0.02694-0.99246-0.81178-0.75000.h5\n",
      "Epoch 25/35\n",
      "23/23 [==============================] - 120s 5s/step - loss: 0.0282 - categorical_accuracy: 0.9913 - val_loss: 0.8076 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00025: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00025-0.02932-0.99095-0.80765-0.75000.h5\n",
      "Epoch 26/35\n",
      "23/23 [==============================] - 114s 5s/step - loss: 0.0166 - categorical_accuracy: 0.9956 - val_loss: 0.8045 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00026: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00026-0.01610-0.99548-0.80454-0.75000.h5\n",
      "Epoch 27/35\n",
      "23/23 [==============================] - 120s 5s/step - loss: 0.0286 - categorical_accuracy: 0.9913 - val_loss: 0.7988 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00027: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00027-0.02927-0.99095-0.79880-0.76000.h5\n",
      "Epoch 28/35\n",
      "23/23 [==============================] - 127s 6s/step - loss: 0.0174 - categorical_accuracy: 0.9971 - val_loss: 0.7945 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00028: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00028-0.01764-0.99698-0.79449-0.77000.h5\n",
      "Epoch 29/35\n",
      "23/23 [==============================] - 112s 5s/step - loss: 0.0275 - categorical_accuracy: 0.9942 - val_loss: 0.7935 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00029: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00029-0.02206-0.99397-0.79352-0.77000.h5\n",
      "Epoch 30/35\n",
      "23/23 [==============================] - 121s 5s/step - loss: 0.0238 - categorical_accuracy: 0.9942 - val_loss: 0.7949 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00030: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00030-0.02374-0.99397-0.79485-0.77000.h5\n",
      "Epoch 31/35\n",
      "23/23 [==============================] - 118s 5s/step - loss: 0.0151 - categorical_accuracy: 0.9971 - val_loss: 0.7938 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00031: saving model to model_conv3d_final_2020-03-3015_03_38.826908/model-00031-0.01573-0.99698-0.79378-0.77000.h5\n",
      "Epoch 32/35\n",
      " 3/23 [==>...........................] - ETA: 2:17 - loss: 0.0170 - categorical_accuracy: 0.9889"
     ]
    }
   ],
   "source": [
    "# Generator parameters\n",
    "num_epochs = 35\n",
    "batch_size = 30\n",
    "frames_per_video = 30\n",
    "frame_red = 1\n",
    "img_x = 120\n",
    "img_y = 120\n",
    "norm = 'minmax'\n",
    "\n",
    "#Model Architecture\n",
    "num_classes = 5\n",
    "\n",
    "#optimizer\n",
    "#opt = Adam(0.001)\n",
    "opt = optimizers.Adadelta()\n",
    "\n",
    "# Work on smaller dataset for initial experiments by randomly pick a subset of t for train v for validation\n",
    "# num_epochs = 20\n",
    "# t,v = 30,20\n",
    "# train_doc = train_doc[ np.random.choice(train_doc.shape[0], t, replace=False)]\n",
    "# val_doc = val_doc[ np.random.choice(val_doc.shape[0], v, replace=False)]\n",
    "# num_train_sequences = len(train_doc)\n",
    "# num_val_sequences = len(val_doc)\n",
    "\n",
    "# Input shape\n",
    "input_shape=(frames_per_video//frame_red,img_x,img_y,3)\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, (3,3,3), input_shape=input_shape, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2))) \n",
    "\n",
    "model.add(Conv3D(32, (1,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, (1,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "            \n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "            \n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile and print the model summary\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n",
    "\n",
    "#checkpoints and callbacks\n",
    "model_name = 'model_conv3d_final' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=10, cooldown=5, verbose=1, mode='auto',min_delta=0.0001)\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "#create the generator for fit_generator\n",
    "train_generator = generator(train_path, train_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "val_generator = generator(val_path, val_doc, batch_size, frames_per_video, frame_red, img_x, img_y)\n",
    "\n",
    "#The steps_per_epoch and validation_steps are used by fit_generator to decide the number of next() calls it need to make.\n",
    "steps_per_epoch = math.ceil(num_train_sequences/batch_size)\n",
    "validation_steps = math.ceil(num_val_sequences/batch_size)\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
